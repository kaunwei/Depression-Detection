{"cells":[{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:28:07.558866Z","iopub.status.busy":"2023-03-23T19:28:07.558513Z","iopub.status.idle":"2023-03-23T19:28:07.565823Z","shell.execute_reply":"2023-03-23T19:28:07.564753Z","shell.execute_reply.started":"2023-03-23T19:28:07.558835Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\shop\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"]}],"source":["import numpy as np\n","import random\n","import json\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","import torch\n","import torch.nn as nn\n","import nltk\n","nltk.download('punkt')\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","from nltk.stem.porter import PorterStemmer\n","stemmer = PorterStemmer()\n"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:13:46.631069Z","iopub.status.busy":"2023-03-23T19:13:46.630691Z","iopub.status.idle":"2023-03-23T19:13:46.638411Z","shell.execute_reply":"2023-03-23T19:13:46.637268Z","shell.execute_reply.started":"2023-03-23T19:13:46.631040Z"},"trusted":true},"outputs":[],"source":["# ANN Model\n","\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.l1 = nn.Linear(input_size, hidden_size) \n","        self.l2 = nn.Linear(hidden_size, hidden_size) \n","        self.l3 = nn.Linear(hidden_size, num_classes)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.relu(out)\n","        out = self.l2(out)\n","        out = self.relu(out)\n","        out = self.l3(out)\n","        return out"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:28:15.058096Z","iopub.status.busy":"2023-03-23T19:28:15.057738Z","iopub.status.idle":"2023-03-23T19:28:15.072184Z","shell.execute_reply":"2023-03-23T19:28:15.071274Z","shell.execute_reply.started":"2023-03-23T19:28:15.058063Z"},"trusted":true},"outputs":[],"source":["with open('intents.json', 'r') as f:\n","    intents = json.load(f)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:28:17.894066Z","iopub.status.busy":"2023-03-23T19:28:17.893675Z","iopub.status.idle":"2023-03-23T19:28:17.902188Z","shell.execute_reply":"2023-03-23T19:28:17.900326Z","shell.execute_reply.started":"2023-03-23T19:28:17.894030Z"},"trusted":true},"outputs":[],"source":["\n","\n","def tokenize(sentence):\n","    \"\"\"\n","    split sentence into array of words/tokens\n","    a token can be a word or punctuation character, or number\n","    \"\"\"\n","    return nltk.word_tokenize(sentence)\n","\n","\n","def stem(word):\n","    \"\"\"\n","    stemming = find the root form of the word\n","    examples:\n","    words = [\"organize\", \"organizes\", \"organizing\"]\n","    words = [stem(w) for w in words]\n","    -> [\"organ\", \"organ\", \"organ\"]\n","    \"\"\"\n","    return stemmer.stem(word.lower())\n","\n","\n","def bag_of_words(tokenized_sentence, words):\n","    \"\"\"\n","    return bag of words array:\n","    1 for each known word that exists in the sentence, 0 otherwise\n","    example:\n","    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n","    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n","    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n","    \"\"\"\n","    # stem each word\n","    sentence_words = [stem(word) for word in tokenized_sentence]\n","    # initialize bag with 0 for each word\n","    bag = np.zeros(len(words), dtype=np.float32)\n","    for idx, w in enumerate(words):\n","        if w in sentence_words: \n","            bag[idx] = 1\n","\n","    return bag"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:28:28.569792Z","iopub.status.busy":"2023-03-23T19:28:28.568982Z","iopub.status.idle":"2023-03-23T19:28:28.622361Z","shell.execute_reply":"2023-03-23T19:28:28.621396Z","shell.execute_reply.started":"2023-03-23T19:28:28.569739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["242 patterns\n","68 tags: ['Hypersomnia', 'anxious_false', 'anxious_true', 'appetite_less', 'appetite_okay', 'appetite_over', 'crying_false', 'crying_true', 'fact-1', 'fact-10', 'fact-11', 'fact-12', 'fact-13', 'fact-14', 'fact-15', 'fact-16', 'fact-17', 'fact-18', 'fact-19', 'fact-2', 'fact-20', 'fact-21', 'fact-22', 'fact-23', 'fact-24', 'fact-25', 'fact-26', 'fact-27', 'fact-28', 'fact-29', 'fact-3', 'fact-30', 'fact-31', 'fact-32', 'fact-33', 'fact-34', 'fact-35', 'fact-36', 'fact-37', 'fact-38', 'fact-39', 'fact-40', 'fact-41', 'fact-42', 'fact-5', 'fact-6', 'fact-7', 'fact-8', 'fact-9', 'feel_bad', 'feel_good', 'good bye', 'greetings', 'harm_false', 'harm_true', 'hopeless_false', 'hopeless_true', 'insomnia', 'joke', 'loneliness_false', 'loneliness_true', 'mentally_tired_false', 'mentally_tired_true', 'proper_sleep', 'relations_bad', 'relations_good', 'smoking_false', 'smoking_true']\n","286 unique stemmed words: [\"'m\", \"'s\", ',', '4', 'a', 'abl', 'about', 'ach', 'activ', 'addict', 'adequ', 'affect', 'after', 'ago', 'all', 'alon', 'aloneless', 'alway', 'am', 'amount', 'an', 'and', 'ani', 'anixeti', 'anxieti', 'anxiou', 'appear', 'appetit', 'approach', 'are', 'around', 'at', 'attack', 'avail', 'away', 'bad', 'be', 'becom', 'bed', 'befor', 'benefit', 'between', 'breadth', 'build', 'bye', 'can', 'cant', 'caus', 'ceil', 'child', 'cigarett', 'comfort', 'compani', 'concentr', 'connect', 'consid', 'consum', 'cope', 'cri', 'cure', 'cut', 'daili', 'day', 'deal', 'defin', 'depress', 'develop', 'diet', 'differ', 'disord', 'distress', 'dizzi', 'do', 'doe', 'doind', 'dont', 'down', 'earli', 'easili', 'eat', 'els', 'enjoy', 'everi', 'excess', 'excessilv', 'exercis', 'exhaust', 'extrovert', 'famili', 'fan', 'fed', 'feel', 'few', 'find', 'fine', 'fit', 'food', 'for', 'frequent', 'friend', 'from', 'front', 'get', 'give', 'go', 'good', 'group', 'happi', 'harm', 'have', 'health', 'hear', 'heavili', 'heeeya', 'hello', 'help', 'hemluuu', 'hi', 'hopeful', 'hopeless', 'how', 'i', 'if', 'ill', 'impact', 'import', 'in', 'initi', 'insomnia', 'intent', 'introvert', 'involv', 'is', 'issu', 'it', 'joke', 'jump', 'knife', 'know', 'larg', 'later', 'learn', 'less', 'life', 'like', 'lone', 'long', 'lot', 'love', 'maintain', 'manag', 'mani', 'me', 'mean', 'medic', 'medit', 'mental', 'mind', 'moment', 'most', 'muscl', 'my', 'myself', 'need', 'neg', 'never', 'new', 'nice', 'no', 'normal', 'not', 'nutrit', 'of', 'off', 'okay', 'on', 'one', 'option', 'or', 'other', 'over', 'overeat', 'oversleep', 'packet', 'palpit', 'parent', 'past', 'peac', 'peopl', 'per', 'physic', 'play', 'pleas', 'poision', 'poor', 'posit', 'prevent', 'problem', 'profession', 'proper', 'properli', 'proprli', 'quatiti', 'quit', 'rare', 'realtion', 'recov', 'relat', 'requir', 'resili', 'right', 'roam', 'role', 'sad', 'see', 'seek', 'self', 'shake', 'short', 'should', 'sign', 'sleep', 'sleepi', 'smoke', 'social', 'some', 'someon', 'sometim', 'stage', 'start', 'strategi', 'stress', 'struggl', 'suffer', 'suicid', 'support', 'sure', 'sweat', 'symptom', 'talk', 'tell', 'tension', 'than', 'the', 'them', 'therapi', 'therapist', 'there', 'thing', 'think', 'thought', 'tie', 'time', 'tio', 'tip', 'tire', 'to', 'toward', 'train', 'treatment', 'trembl', 'tri', 'type', 'unecessarili', 'unhealthi', 'unwel', 'up', 'vein', 'veri', 'wa', 'want', 'warn', 'week', 'well', 'well-b', 'what', 'where', 'whi', 'who', 'will', 'with', 'wonderful', 'worri', 'ya', 'ye', 'you']\n"]}],"source":["all_words = []\n","tags = []\n","xy = []\n","# loop through each sentence in our intents patterns\n","for intent in intents['intents']:\n","    tag = intent['tag']\n","    # add to tag list\n","    tags.append(tag)\n","    for pattern in intent['patterns']:\n","        # tokenize each word in the sentence\n","        w = tokenize(pattern)\n","        # add to our words list\n","        all_words.extend(w)\n","        # add to xy pair\n","        xy.append((w, tag))\n","\n","# stem and lower each word\n","ignore_words = ['?', '.', '!']\n","all_words = [stem(w) for w in all_words if w not in ignore_words]\n","# remove duplicates and sort\n","all_words = sorted(set(all_words))\n","tags = sorted(set(tags))\n","\n","print(len(xy), \"patterns\")\n","print(len(tags), \"tags:\", tags)\n","print(len(all_words), \"unique stemmed words:\", all_words)\n","\n"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:28:33.065839Z","iopub.status.busy":"2023-03-23T19:28:33.065239Z","iopub.status.idle":"2023-03-23T19:28:33.104741Z","shell.execute_reply":"2023-03-23T19:28:33.103120Z","shell.execute_reply.started":"2023-03-23T19:28:33.065810Z"},"trusted":true},"outputs":[],"source":["# create training data\n","X_train = []\n","y_train = []\n","for (pattern_sentence, tag) in xy:\n","    # X: bag of words for each pattern_sentence\n","    bag = bag_of_words(pattern_sentence, all_words)\n","    X_train.append(bag)\n","    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n","    label = tags.index(tag)\n","    y_train.append(label)\n","\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:28:47.742326Z","iopub.status.busy":"2023-03-23T19:28:47.741954Z","iopub.status.idle":"2023-03-23T19:28:47.748776Z","shell.execute_reply":"2023-03-23T19:28:47.747676Z","shell.execute_reply.started":"2023-03-23T19:28:47.742294Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["286 68\n"]}],"source":["# Hyper-parameters \n","num_epochs = 200\n","batch_size = 32\n","learning_rate = 0.001\n","input_size = len(X_train[0])\n","hidden_size = 8\n","output_size = len(tags)\n","print(input_size, output_size)\n","\n"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:29:05.741418Z","iopub.status.busy":"2023-03-23T19:29:05.740216Z","iopub.status.idle":"2023-03-23T19:29:05.747491Z","shell.execute_reply":"2023-03-23T19:29:05.746500Z","shell.execute_reply.started":"2023-03-23T19:29:05.741337Z"},"trusted":true},"outputs":[],"source":["class ChatDataset(Dataset):\n","\n","    def __init__(self):\n","        self.n_samples = len(X_train)\n","        self.x_data = X_train\n","        self.y_data = y_train\n","\n","    # support indexing such that dataset[i] can be used to get i-th sample\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]\n","\n","    # we can call len(dataset) to return the size\n","    def __len__(self):\n","        return self.n_samples\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:29:31.950117Z","iopub.status.busy":"2023-03-23T19:29:31.949708Z","iopub.status.idle":"2023-03-23T19:29:31.991232Z","shell.execute_reply":"2023-03-23T19:29:31.990312Z","shell.execute_reply.started":"2023-03-23T19:29:31.950084Z"},"trusted":true},"outputs":[],"source":["dataset = ChatDataset()\n","train_loader = DataLoader(dataset=dataset,\n","                          batch_size=batch_size,\n","                          shuffle=True,\n","                          num_workers=0)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = NeuralNet(input_size, hidden_size, output_size).to(device)\n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/200], Loss: 4.2590, Accuracy: 0.41%\n","Epoch [2/200], Loss: 4.2505, Accuracy: 0.41%\n","Epoch [3/200], Loss: 4.2302, Accuracy: 2.48%\n","Epoch [4/200], Loss: 4.2258, Accuracy: 3.72%\n","Epoch [5/200], Loss: 4.2106, Accuracy: 3.72%\n","Epoch [6/200], Loss: 4.2005, Accuracy: 4.13%\n","Epoch [7/200], Loss: 4.1937, Accuracy: 4.13%\n","Epoch [8/200], Loss: 4.1708, Accuracy: 4.55%\n","Epoch [9/200], Loss: 4.1628, Accuracy: 4.55%\n","Epoch [10/200], Loss: 4.1521, Accuracy: 4.55%\n","Epoch [11/200], Loss: 4.1307, Accuracy: 4.55%\n","Epoch [12/200], Loss: 4.1022, Accuracy: 4.96%\n","Epoch [13/200], Loss: 4.0832, Accuracy: 4.96%\n","Epoch [14/200], Loss: 4.0578, Accuracy: 4.55%\n","Epoch [15/200], Loss: 4.0276, Accuracy: 4.55%\n","Epoch [16/200], Loss: 3.9953, Accuracy: 4.55%\n","Epoch [17/200], Loss: 3.9696, Accuracy: 4.55%\n","Epoch [18/200], Loss: 3.9283, Accuracy: 4.55%\n","Epoch [19/200], Loss: 3.9010, Accuracy: 4.96%\n","Epoch [20/200], Loss: 3.8608, Accuracy: 5.37%\n","Epoch [21/200], Loss: 3.8098, Accuracy: 7.02%\n","Epoch [22/200], Loss: 3.7711, Accuracy: 6.61%\n","Epoch [23/200], Loss: 3.7280, Accuracy: 8.26%\n","Epoch [24/200], Loss: 3.6881, Accuracy: 12.40%\n","Epoch [25/200], Loss: 3.6651, Accuracy: 13.22%\n","Epoch [26/200], Loss: 3.6160, Accuracy: 13.64%\n","Epoch [27/200], Loss: 3.5729, Accuracy: 14.46%\n","Epoch [28/200], Loss: 3.5178, Accuracy: 14.05%\n","Epoch [29/200], Loss: 3.4919, Accuracy: 15.70%\n","Epoch [30/200], Loss: 3.4499, Accuracy: 13.64%\n","Epoch [31/200], Loss: 3.4414, Accuracy: 11.57%\n","Epoch [32/200], Loss: 3.3780, Accuracy: 10.33%\n","Epoch [33/200], Loss: 3.3397, Accuracy: 9.92%\n","Epoch [34/200], Loss: 3.3242, Accuracy: 9.92%\n","Epoch [35/200], Loss: 3.2895, Accuracy: 11.57%\n","Epoch [36/200], Loss: 3.2571, Accuracy: 12.40%\n","Epoch [37/200], Loss: 3.2158, Accuracy: 13.22%\n","Epoch [38/200], Loss: 3.1883, Accuracy: 15.70%\n","Epoch [39/200], Loss: 3.1289, Accuracy: 16.53%\n","Epoch [40/200], Loss: 3.1051, Accuracy: 17.36%\n","Epoch [41/200], Loss: 3.0631, Accuracy: 18.60%\n","Epoch [42/200], Loss: 3.0452, Accuracy: 19.01%\n","Epoch [43/200], Loss: 3.0238, Accuracy: 20.66%\n","Epoch [44/200], Loss: 3.0005, Accuracy: 21.90%\n","Epoch [45/200], Loss: 2.9370, Accuracy: 22.31%\n","Epoch [46/200], Loss: 2.8981, Accuracy: 23.97%\n","Epoch [47/200], Loss: 2.8893, Accuracy: 26.86%\n","Epoch [48/200], Loss: 2.8343, Accuracy: 27.69%\n","Epoch [49/200], Loss: 2.8074, Accuracy: 28.10%\n","Epoch [50/200], Loss: 2.7689, Accuracy: 27.69%\n","Epoch [51/200], Loss: 2.7476, Accuracy: 28.51%\n","Epoch [52/200], Loss: 2.7114, Accuracy: 28.51%\n","Epoch [53/200], Loss: 2.6759, Accuracy: 29.34%\n","Epoch [54/200], Loss: 2.6494, Accuracy: 30.17%\n","Epoch [55/200], Loss: 2.6121, Accuracy: 31.82%\n","Epoch [56/200], Loss: 2.5844, Accuracy: 32.23%\n","Epoch [57/200], Loss: 2.5793, Accuracy: 33.06%\n","Epoch [58/200], Loss: 2.5415, Accuracy: 33.88%\n","Epoch [59/200], Loss: 2.5262, Accuracy: 35.54%\n","Epoch [60/200], Loss: 2.4809, Accuracy: 35.54%\n","Epoch [61/200], Loss: 2.4463, Accuracy: 36.78%\n","Epoch [62/200], Loss: 2.4035, Accuracy: 37.19%\n","Epoch [63/200], Loss: 2.4016, Accuracy: 38.84%\n","Epoch [64/200], Loss: 2.3630, Accuracy: 40.08%\n","Epoch [65/200], Loss: 2.3291, Accuracy: 40.50%\n","Epoch [66/200], Loss: 2.2918, Accuracy: 40.50%\n","Epoch [67/200], Loss: 2.2946, Accuracy: 40.50%\n","Epoch [68/200], Loss: 2.2557, Accuracy: 42.15%\n","Epoch [69/200], Loss: 2.2363, Accuracy: 44.21%\n","Epoch [70/200], Loss: 2.2002, Accuracy: 45.04%\n","Epoch [71/200], Loss: 2.1808, Accuracy: 43.80%\n","Epoch [72/200], Loss: 2.1555, Accuracy: 45.04%\n","Epoch [73/200], Loss: 2.1437, Accuracy: 47.52%\n","Epoch [74/200], Loss: 2.1059, Accuracy: 46.69%\n","Epoch [75/200], Loss: 2.0881, Accuracy: 47.11%\n","Epoch [76/200], Loss: 2.0522, Accuracy: 49.59%\n","Epoch [77/200], Loss: 2.0113, Accuracy: 50.83%\n","Epoch [78/200], Loss: 2.0180, Accuracy: 51.65%\n","Epoch [79/200], Loss: 1.9762, Accuracy: 52.89%\n","Epoch [80/200], Loss: 1.9600, Accuracy: 53.31%\n","Epoch [81/200], Loss: 1.9348, Accuracy: 53.72%\n","Epoch [82/200], Loss: 1.9079, Accuracy: 54.13%\n","Epoch [83/200], Loss: 1.8785, Accuracy: 54.55%\n","Epoch [84/200], Loss: 1.8440, Accuracy: 54.55%\n","Epoch [85/200], Loss: 1.8267, Accuracy: 55.37%\n","Epoch [86/200], Loss: 1.8016, Accuracy: 56.20%\n","Epoch [87/200], Loss: 1.7641, Accuracy: 57.85%\n","Epoch [88/200], Loss: 1.7432, Accuracy: 58.26%\n","Epoch [89/200], Loss: 1.7473, Accuracy: 59.09%\n","Epoch [90/200], Loss: 1.6890, Accuracy: 60.74%\n","Epoch [91/200], Loss: 1.6765, Accuracy: 62.40%\n","Epoch [92/200], Loss: 1.6676, Accuracy: 64.88%\n","Epoch [93/200], Loss: 1.6466, Accuracy: 63.64%\n","Epoch [94/200], Loss: 1.6204, Accuracy: 64.05%\n","Epoch [95/200], Loss: 1.5935, Accuracy: 67.77%\n","Epoch [96/200], Loss: 1.5758, Accuracy: 67.77%\n","Epoch [97/200], Loss: 1.5317, Accuracy: 69.83%\n","Epoch [98/200], Loss: 1.5428, Accuracy: 69.83%\n","Epoch [99/200], Loss: 1.4997, Accuracy: 71.07%\n","Epoch [100/200], Loss: 1.4767, Accuracy: 71.07%\n","Epoch [101/200], Loss: 1.4686, Accuracy: 72.31%\n","Epoch [102/200], Loss: 1.4156, Accuracy: 73.97%\n","Epoch [103/200], Loss: 1.4046, Accuracy: 74.38%\n","Epoch [104/200], Loss: 1.3664, Accuracy: 74.38%\n","Epoch [105/200], Loss: 1.3630, Accuracy: 76.86%\n","Epoch [106/200], Loss: 1.3301, Accuracy: 77.69%\n","Epoch [107/200], Loss: 1.3187, Accuracy: 78.51%\n","Epoch [108/200], Loss: 1.2803, Accuracy: 80.17%\n","Epoch [109/200], Loss: 1.2622, Accuracy: 80.58%\n","Epoch [110/200], Loss: 1.2503, Accuracy: 80.58%\n","Epoch [111/200], Loss: 1.2486, Accuracy: 81.40%\n","Epoch [112/200], Loss: 1.2058, Accuracy: 80.99%\n","Epoch [113/200], Loss: 1.1966, Accuracy: 81.40%\n","Epoch [114/200], Loss: 1.1496, Accuracy: 82.23%\n","Epoch [115/200], Loss: 1.1414, Accuracy: 82.23%\n","Epoch [116/200], Loss: 1.1267, Accuracy: 82.64%\n","Epoch [117/200], Loss: 1.0957, Accuracy: 83.06%\n","Epoch [118/200], Loss: 1.0958, Accuracy: 83.47%\n","Epoch [119/200], Loss: 1.0616, Accuracy: 83.06%\n","Epoch [120/200], Loss: 1.0639, Accuracy: 84.30%\n","Epoch [121/200], Loss: 1.0266, Accuracy: 84.30%\n","Epoch [122/200], Loss: 1.0080, Accuracy: 85.12%\n","Epoch [123/200], Loss: 0.9748, Accuracy: 85.12%\n","Epoch [124/200], Loss: 0.9754, Accuracy: 84.71%\n","Epoch [125/200], Loss: 0.9704, Accuracy: 84.71%\n","Epoch [126/200], Loss: 0.9412, Accuracy: 85.54%\n","Epoch [127/200], Loss: 0.9121, Accuracy: 85.54%\n","Epoch [128/200], Loss: 0.9075, Accuracy: 85.54%\n","Epoch [129/200], Loss: 0.8804, Accuracy: 86.36%\n","Epoch [130/200], Loss: 0.8716, Accuracy: 85.95%\n","Epoch [131/200], Loss: 0.8595, Accuracy: 86.78%\n","Epoch [132/200], Loss: 0.8454, Accuracy: 86.78%\n","Epoch [133/200], Loss: 0.8165, Accuracy: 87.19%\n","Epoch [134/200], Loss: 0.8008, Accuracy: 87.60%\n","Epoch [135/200], Loss: 0.7805, Accuracy: 87.19%\n","Epoch [136/200], Loss: 0.7824, Accuracy: 87.60%\n","Epoch [137/200], Loss: 0.7802, Accuracy: 88.02%\n","Epoch [138/200], Loss: 0.7683, Accuracy: 88.43%\n","Epoch [139/200], Loss: 0.7405, Accuracy: 88.84%\n","Epoch [140/200], Loss: 0.7187, Accuracy: 88.84%\n","Epoch [141/200], Loss: 0.7135, Accuracy: 88.43%\n","Epoch [142/200], Loss: 0.6987, Accuracy: 88.43%\n","Epoch [143/200], Loss: 0.6919, Accuracy: 89.26%\n","Epoch [144/200], Loss: 0.6972, Accuracy: 90.08%\n","Epoch [145/200], Loss: 0.6582, Accuracy: 89.67%\n","Epoch [146/200], Loss: 0.6482, Accuracy: 89.67%\n","Epoch [147/200], Loss: 0.6489, Accuracy: 90.08%\n","Epoch [148/200], Loss: 0.6269, Accuracy: 89.67%\n","Epoch [149/200], Loss: 0.6310, Accuracy: 90.50%\n","Epoch [150/200], Loss: 0.6087, Accuracy: 91.32%\n","Epoch [151/200], Loss: 0.5881, Accuracy: 91.32%\n","Epoch [152/200], Loss: 0.5822, Accuracy: 91.32%\n","Epoch [153/200], Loss: 0.5731, Accuracy: 91.32%\n","Epoch [154/200], Loss: 0.5700, Accuracy: 91.32%\n","Epoch [155/200], Loss: 0.5588, Accuracy: 91.74%\n","Epoch [156/200], Loss: 0.5457, Accuracy: 92.15%\n","Epoch [157/200], Loss: 0.5350, Accuracy: 92.56%\n","Epoch [158/200], Loss: 0.5254, Accuracy: 92.98%\n","Epoch [159/200], Loss: 0.5086, Accuracy: 93.80%\n","Epoch [160/200], Loss: 0.5077, Accuracy: 93.80%\n","Epoch [161/200], Loss: 0.5027, Accuracy: 93.80%\n","Epoch [162/200], Loss: 0.4840, Accuracy: 94.63%\n","Epoch [163/200], Loss: 0.4792, Accuracy: 93.80%\n","Epoch [164/200], Loss: 0.4735, Accuracy: 94.63%\n","Epoch [165/200], Loss: 0.4721, Accuracy: 95.87%\n","Epoch [166/200], Loss: 0.4615, Accuracy: 96.69%\n","Epoch [167/200], Loss: 0.4652, Accuracy: 96.69%\n","Epoch [168/200], Loss: 0.4426, Accuracy: 96.28%\n","Epoch [169/200], Loss: 0.4372, Accuracy: 96.28%\n","Epoch [170/200], Loss: 0.4238, Accuracy: 96.69%\n","Epoch [171/200], Loss: 0.4219, Accuracy: 97.11%\n","Epoch [172/200], Loss: 0.4227, Accuracy: 97.11%\n","Epoch [173/200], Loss: 0.4091, Accuracy: 97.52%\n","Epoch [174/200], Loss: 0.3977, Accuracy: 97.52%\n","Epoch [175/200], Loss: 0.3876, Accuracy: 97.52%\n","Epoch [176/200], Loss: 0.3831, Accuracy: 97.52%\n","Epoch [177/200], Loss: 0.3824, Accuracy: 97.52%\n","Epoch [178/200], Loss: 0.3720, Accuracy: 97.52%\n","Epoch [179/200], Loss: 0.3712, Accuracy: 97.52%\n","Epoch [180/200], Loss: 0.3638, Accuracy: 97.52%\n","Epoch [181/200], Loss: 0.3487, Accuracy: 97.52%\n","Epoch [182/200], Loss: 0.3547, Accuracy: 98.35%\n","Epoch [183/200], Loss: 0.3421, Accuracy: 98.35%\n","Epoch [184/200], Loss: 0.3328, Accuracy: 98.35%\n","Epoch [185/200], Loss: 0.3319, Accuracy: 98.35%\n","Epoch [186/200], Loss: 0.3332, Accuracy: 98.35%\n","Epoch [187/200], Loss: 0.3244, Accuracy: 98.35%\n","Epoch [188/200], Loss: 0.3148, Accuracy: 98.76%\n","Epoch [189/200], Loss: 0.3059, Accuracy: 98.35%\n","Epoch [190/200], Loss: 0.3106, Accuracy: 98.35%\n","Epoch [191/200], Loss: 0.3044, Accuracy: 98.35%\n","Epoch [192/200], Loss: 0.2922, Accuracy: 98.35%\n","Epoch [193/200], Loss: 0.2895, Accuracy: 98.35%\n","Epoch [194/200], Loss: 0.2802, Accuracy: 98.35%\n","Epoch [195/200], Loss: 0.2785, Accuracy: 98.35%\n","Epoch [196/200], Loss: 0.2797, Accuracy: 98.35%\n","Epoch [197/200], Loss: 0.2728, Accuracy: 98.35%\n","Epoch [198/200], Loss: 0.2721, Accuracy: 98.35%\n","Epoch [199/200], Loss: 0.2686, Accuracy: 98.76%\n","Epoch [200/200], Loss: 0.2630, Accuracy: 98.35%\n"]}],"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for (words, labels) in train_loader:\n","        words = words.to(device)\n","        labels = labels.to(dtype=torch.long).to(device)\n","        \n","        # Forward pass\n","        outputs = model(words)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track accuracy\n","        _, predicted_labels = torch.max(outputs, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","        total_loss += loss.item()\n","\n","    average_loss = total_loss / len(train_loader)\n","    accuracy = correct_predictions / total_samples\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}, Accuracy: {accuracy:.2%}')\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:29:52.139534Z","iopub.status.busy":"2023-03-23T19:29:52.139168Z","iopub.status.idle":"2023-03-23T19:30:12.702025Z","shell.execute_reply":"2023-03-23T19:30:12.700821Z","shell.execute_reply.started":"2023-03-23T19:29:52.139504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [100/200], Loss: 0.0386, Accuracy: 100.00%\n","Epoch [200/200], Loss: 0.0101, Accuracy: 100.00%\n","Final loss: 0.0101, Final accuracy: 100.00%\n"]}],"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for (words, labels) in train_loader:\n","        words = words.to(device)\n","        labels = labels.to(dtype=torch.long).to(device)\n","        \n","        # Forward pass\n","        outputs = model(words)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track accuracy\n","        _, predicted_labels = torch.max(outputs, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","        total_loss += loss.item()\n","\n","    average_loss = total_loss / len(train_loader)\n","    accuracy = correct_predictions / total_samples\n","\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}, Accuracy: {accuracy:.2%}')\n","\n","print(f'Final loss: {average_loss:.4f}, Final accuracy: {accuracy:.2%}')\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:30:22.689364Z","iopub.status.busy":"2023-03-23T19:30:22.688954Z","iopub.status.idle":"2023-03-23T19:30:22.697765Z","shell.execute_reply":"2023-03-23T19:30:22.696426Z","shell.execute_reply.started":"2023-03-23T19:30:22.689307Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["training complete. file saved to data.pth\n"]}],"source":["data = {\n","\"model_state\": model.state_dict(),\n","\"input_size\": input_size,\n","\"hidden_size\": hidden_size,\n","\"output_size\": output_size,\n","\"all_words\": all_words,\n","\"tags\": tags\n","}\n","\n","FILE = \"data.pth\"\n","torch.save(data, FILE)\n","\n","print(f'training complete. file saved to {FILE}')"]},{"cell_type":"markdown","metadata":{},"source":["## Chat with chatbot for testing"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-03-23T19:32:28.242686Z","iopub.status.busy":"2023-03-23T19:32:28.242196Z","iopub.status.idle":"2023-03-23T19:33:24.148374Z","shell.execute_reply":"2023-03-23T19:33:24.147381Z","shell.execute_reply.started":"2023-03-23T19:32:28.242642Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Let's chat! (type 'quit' to exit)\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","\n","FILE = \"data.pth\"\n","data = torch.load(FILE)\n","\n","input_size = data[\"input_size\"]\n","hidden_size = data[\"hidden_size\"]\n","output_size = data[\"output_size\"]\n","all_words = data['all_words']\n","tags = data['tags']\n","model_state = data[\"model_state\"]\n","\n","model = NeuralNet(input_size, hidden_size, output_size).to(device)\n","model.load_state_dict(model_state)\n","model.eval()\n","\n","bot_name = \"Medical ChatBot\"\n","print(\"Let's chat! (type 'quit' to exit)\")\n","while True:\n","    # sentence = \"do you use credit cards?\"\n","    sentence = input(\"You: \")\n","    if sentence == \"quit\":\n","        break\n","\n","    sentence = tokenize(sentence)\n","    X = bag_of_words(sentence, all_words)\n","    X = X.reshape(1, X.shape[0])\n","    X = torch.from_numpy(X).to(device)\n","\n","    output = model(X)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    tag = tags[predicted.item()]\n","\n","    probs = torch.softmax(output, dim=1)\n","    prob = probs[0][predicted.item()]\n","    if prob.item() > 0.75:\n","        for intent in intents['intents']:\n","            if tag == intent[\"tag\"]:\n","                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n","    else:\n","        print(f\"{bot_name}: I do not understand...\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
